%=========================================================================

\chapter{Introduction}

Memory usage is one of the most fundamental principles in computing. Every program needs some storage - if only for storing itself somewhere. Modern computing environments contain lot of memory layers on different levels. There is persistent storage (harddisks, SDD drives), operating memory, various levels of CPU caches and registers. Usage of all of these is usually abstracted by the operating systems, which provides the userspace applications safe access to these resources. Technical advancement in recent age caused the prices of hardware providing storage to go down, so in most causes, totally effective usage of these resources is not an outstanding issue when programming. The storage is usually relatively cheap to the price of work an expert programmer has to do to exploit the resources effectively. However, sometimes the effectiveness matters. This thesis attempts to provide a tool for an effective work with dynamically allocated program memory.

\section{Motivation} 

Dynamically allocated memory is a memory allocated not during compile time, but on runtime. The reasons vary - the amount of memory needed can be computed from variables, or the memory is needed just for the limited amount of time and so it would be ineffective to hold it for the whole runtime. The concept is almost as old as modern computing itself. It seems to be present and used since the 1960s.\cite{DSAsurvey} In C language, which is still mostly used in system programming in Linux world, this concept is present in the omnipresent malloc/free interface.\cite{glibc-man-malloc} Of course, this interface is present and widely used for a long time, so the concepts are already researched thoroughly. There are some very efficient allocation algorithms in currently used implementations of malloc. There seems to be little space for some radical improvements.

However, the effectiveness of memory allocator is also largely influenced by the usage pattern of it. Over time, the most used memory allocator implementations converged towards an implementation which has at least an acceptable behavior for most of the usage cases. This means that for most cases, a program has a good probability to perform well with the allocator, but it could do better with another. Most widely used allocators often provide a way for fine tuning them for the particular environment\cite{glibc-man-malloc}, but often the results are not as good as they could be if the allocator was made specifically for the specific usage pattern, sacrificing performance for another cases.

The good example of this case is the program concurrence. For handling multiple threads allocating memory concurrently, the allocator has to have some mechanism avoiding conflicts, like locks. Stock allocator does not know whether program using it is concurrent or not. A lot of programs use threads, so it needs to have some overhead for them. This overhead is completely unnecessary for single-threaded programs, and better performance would probable be achieved if an allocator without the multithreaded overhead was used. Such an allocator of course could not be used as system-wide, universal one. All of this is of course more complicated in reality.

The environment also changes in time. The appearance of multi-core CPUs brought some specific problems with cache usage. It is better to allocate memory in a way that is cache friendly, that means to arrange memory which is used closely in time also close spatially, and to avoid writing into memory which is shared among the CPU caches. When CPUs invalidate each other's caches a lot, the advantage of having it is lost. Such problems can be avoided with proper memory management.

The advantage of using tuned or alternative memory allocator is often not great enough to justify additional work. Stock allocators are quite effective in most cases. But there are cases in which the gain can be quite high. High enough so we can see a major software gigant creating it's own malloc implementation to solve their problems with existing implementation.\cite{tcmalloc} Another cases can be seen in some object-oriented libraries, which implement additional layer of memory management. The reason is that creating objects does quite a large number of small allocations, which is not effective enough. So these libraries keep a pool of preallocated objects on which they do some amount of memory allocation of their own.\cite{glib-memalloc}

This thesis aims to help with analysing performance of various memory allocator implementations (and their tunings) by providing a tool which can measure different performance aspects under various conditions. At the moment, such an analysis seems to be usually done by micro benchmarks crafted for the actual need without systematic approach, at least in the open-source software world.

\section{Goals}

The goal of this thesis is to provide a tool for analysing memory allocator performance. Allocator performance is a vague term - different metrics can be claimed as important in different situations. The tool should be able to measure all of the most common. These will have to be identified. Also, there are many factors which influence these metrics. The tool shall allow the user to specify these factors to make the measurement relevant for the particular situation.

After the identifying these factors and metrics, the tool will be implemented and test analysis will be performed on various memory allocation implementations.

\chapter{Dynamic memory allocators}

This chapter discusses the principles of dynamic memory allocation system used in modern GNU/Linux systems. Because {\em performance} is very unclear term when used in context of allocators, various metrics are analysed as performance ones, and factors which influence these metrics are discovered.

\section{Dynamic memory allocation}

Dynamic memory allocation is an ubiquitous concept in current programming world. It originated in the very dawn of modern computing, in 1960s, and was driven by the need of using the memory resources as effectivelly as possible. Dynamic memory allocation occurs at run-time, where a running program asks the operating system (or more generally, the memory resource manager) for more memory for its needs. This is a direct opposite of compile-time {\em static} allocation. There are more reasons for postponing allocation to runtime. The most significant ones are:
\begin{enumerate}
\item The exact amount of needed memory is simply not known during compile-time. This is the common case: the program simply does not know how long e.g. the input will be.
\item The program needs various amounts of memory in time. Such a memory could possibly be allocated at compile-time if the size is known: simply the maximum of the memory consumption would be allocated. That would mean wasting the memory, because the program would have allocated a lot of memory even when it uses all of it for only a small portion of runtime.
\end{enumerate}

\subsection{DSA specification}

In the scope of this thesis, the term "dynamic memory allocator" means the usual malloc/free interface as it is known from the C programming language. The programs use the allocator for obtaining and returning arbitrary amount of storage space for its own use. These allocations and allocations can occur at any time and in any order. Allocated memory can be directly addressed by the program - the access to the storage is done outside of the allocator scope. The allocator is also completely unaware about what is stored in the allocated memory. This means that the allocator cannot do any internal management on already allocated blocks of memory, e.g. moving the allocated blocks so that they are stored compactly, or possibly compressing them. If such operations were done by the allocator, the program references to such memory blocks would be rendered invalid, and such an effect is of course indesirable. The direct access also means that the allocated memory chunk needs to be contiguous, as opposed to e.g. filesystem storage management. A program cannot recognize the hole in the storage without the allocator's assistance - so the hole simply can not exist.

\subsection{DSA concepts}

The allocator operates on some amount of memory, which is usually (but not necessarilly) contiguous. Some part of this memory is used for the memory allocator data - these will be called metadata later in this thesis. The rest of this memory is used for provisioning to a program, and is usually divided into some number of blocks of various size. When a program makes a request for some amount of memory, the allocator finds or creates some free block of sufficient size, marks it as used and gives it's address to the program. If no large enough block is found, additional memory can be obtained from the operating system using appropriate system calls, such as {\em mmap}, {\em brk} or {\em sbrk}.\cite{syscalls} When a memory is freed by the program, the block is marked as free. Free blocks may be used to satisfy subsequent memory request, or it may be returned to the operating system. The goal of an allocator is to minimize the unallocated memory amount, while minimizing also the time cost of this effort.

There are few notable constraints which makes the allocator's goal harder. The allocator cannot control the number, order and the memory size of allocations - these are set by a program using the allocator. The allocator also satisfies one allocation at a time (with the exception of possible concurrency, which does not affect this problem), without the knowledge about the future allocations requests. The memory must be provided immediatelly, and once the allocator selects the memory place for it, it cannot be modified when additional requests come and better decision could be made, until the program frees that memory. 

\section{Interesting performance metrics of memory allocators}

There is no clear performance metric for the memory allocators. There are different metrics describing the characteristics of an allocator, and their importance and relevance as a performance metric depends on the characteristics of a program using an allocator. For example, a time per allocation certainly matters in a program doing millions of allocations, while it is not much interesting in a program with only few allocations. This section lists various metrics which can be under some circumstances considered as relevant performance metrics, along with these circumstances.

There are two types of performance metrics: time and memory usage. Time metrics affect the program execution speed in some way. Two time-affecting metrics were identified: allocation speed, which directly affects the program speed, and locality, which affects the program speed indirectly by making the program more or less friendly to CPU cache systems. Memory metrics are also two: fragmentation and memory overhead. Memory overhead increases memory usage by using some memory for the allocator data. Fragmentation affects the efficiency of using memory for provisioning to a program.

\subsection{Memory overhead}
\label{metrics:overhead}
Every memory allocator needs some memory for it's own use. At the very least, it needs to store the sizes of the memory blocks provided to program, so they can be marked as unused when freed. There are two possible types of memory overhead: per allocator, and per allocation. Overhead per allocator means a memory consumption by {\em global} data structures used by the allocator. This memory will be used even when no memory is allocated by the program. Some allocators need some metadata memory for each allocation made by the program. This is per-allocation overhead. The former is not that important, if it is not extremely large. The latter can be important under certain conditions. One of the cases when per-allocation overhead is interesting metric is obviously when program does a large amount of very small allocations. For example, when per-allocation metadata needs 16 bytes and the program does a lot of 16 bytes allocations, the amount of memory needed for such program is twice as high as the amount actually used by it. There are more reasons why the allocator can create some overhead memory: in addition to the memory actually used by allocator metadata the memory overhead doesn't necessarily need to be used at all. The free unused memory block can be created when some architecture dependent decisions are made, for example alignment. A figure \ref{whatever} shows the different types of memory overhead. Memory overhead is sometimes considered to be a type of fragmentation. Fragmentation is discussed in more details in \ref{metrics:fragmentation}.

\subsection{Allocation speed}

Allocation speed is simply the amount of time spent processing the allocation or deallocation request. In usual use cases, the allocation speed is not very important, because allocator requests usually make only a minor portion of program statements. In such cases, time taken by memory allocation requests is negligible in the total time the program is executing (if the time taken by allocator is not extreme, of course), so the performance impact in the program is low. However, this is not always the case. In some environments where a large number of allocations and deallocations occurs, the program performance can rapidly increase and decrease in accordance to the allocator performance. An example of such environment is the interpreter of some object-oriented scriptiong language, such as Python or Ruby. When these interpreters are processing some scripts, large hierarchies of objects can be created or destroyed (for example when entering or leaving a function), which results in a large chain of small allocations or deallocations. Low performance allocator can rapidly decrease performance of execution of such a program.

\subsection{Locality}

Locality is a metric which can dramatically affect performance of a program. Current computers have several layers of operating memory - in addition to "normal" operating memory, current CPUs have several levels of cache memory. For example, Intel i7 family of CPUs have three levels of cache. Usually, the closer the cache level is to the CPU itself, the faster the operations with data in it. Reading data from L1 cache can be X times\cite{drepper-memory} faster than reading same data from the main memory. Smart memory allocator can arrange data in a way that is friendly to this memory structure: if data, which are often used together are also placed near themselves in memory, then it is likely that both can be present in the fastest cache, and thus all the work can be done on the fast cache level without the need of multiple main memory accesses. Similarly, poor allocator can place data which are used together in different memory areas, which then basically eradicates the advantage of having a cache because of cache trashing. 

Allocators can improve their locality metric either by taking advantage of some usual program patterns, or by taking hints from the program it serves. One quite common pattern is a temporal one. Data which are allocated close in time are quite likely to be used close in time. So if an allocator places temporally close allocations also spatially close, it has a good chance of being cache-friendly, improving the program's performance. Second method is taking hints from the program itself - the programmer usually knows which data will be used together. 

\subsection{Fragmentation}
\label{metrics:fragmentation}

Fragmentation is a metric representing the effectivity with which the allocator manages it's memory. The allocator has some amount of memory disposable. The difference between this amount and the amount actually used (as in provided to the program, it's not important if the program actually uses its allocated memory) by the program gives us fragmentation. Fragmentation is usually taken in a form of fragmentation ratio or percentage, which is computed like $unused\_memory / total\_memory$ and says how much of the memory is not used. The goal for an allocator is to have this metric as low as possible.

In literature, fragmentation is often divided into two categories: internal and external.
\begin{enumerate}
\item Internal fragmentation is memory actually used for something, but not accessible to the program. Internal fragmentation was discussed in \ref{metrics:overhead} and in this thesis the term {\em memory overhead} is used for this type of fragmentation.
\item External fragmentation is memory, which is for some reason currently unallocated by the program. In his thesis, the term {\em fragmentation} is used for this kind of fragmentation. The reason for dividing these two types with distinct names is an attempt to avoid confusion. This thesis works with both these metrics in different ways, because they are affected by different factors. This subsection later deals just with this metric.
\end{enumerate}

The reasons why fragmentation come up can be different. It can be a memory already obtained from the operating system, but not yet allocated by the memory, for example at the start of the program. This can be a case where an allocator is handling lot of small allocation requests - explicitly asking the operating system for tiny amount of memory is unefficient: the syscalls are usually quite expensive. Therefore, allocator requests a larger block at the beginning and uses pieces of this block to satisfy the allocation requests. Also, there can exist a situation where free memory exists when an allocation request is made, but for some reason cannot be used to satisfy it. The reasons for this vary: the block can be free space between two used blocks, too small for the allocation request to be satisfied. Or, in a multithreaded environment, allocator can have a sufficiently large free block of memory, but it doesn't use it because it only uses that memory chunk for some exact thread, not the one requesting the memory.

The problem with fragmentation is that it is not a proper feature of an allocator. The major factor affecting fragmentation is how the allocator is used, i.e. size and order of the allocation requests. For some work loads fragmentation of a certain allocator can be low, and high for other. Two allocation scenarios on a simple heap-based allocator illustrate this. Simple heap-based allocator simply grows the heap if asked for memory, and shrinks it if asked to free memory chunk at the top of the heap. For allocation requests coming as {\tt malloc} paired with subsequent {\tt free} of the same memory, no fragmentation would occur. The heap would grow with {\tt malloc} and then it would shrink after free. But after the sequence {\tt malloc(ptr1, 100), malloc(ptr2, 5), free(ptr1)} there is large fragmentation: the heap size is 105, but only a memory chunk with size 5 is actually used. Note that this is not so uncommon pattern in software. A similar behaviour could be observed for a program part looking like this: {\em allocate space for data; allocate space for result; do some work on data; store the result; free the data; use the result}. The behavior of the described allocator on these two scenarios is showed on figure \ref{whateverasdfds}.

It can be clearly seen that fragmentation can vary even in different times during the execution of a single program. This can be a problem especially for a long running programs doing lot of allocations and deallocations, like daemons or web browsers. If a bad memory allocator is used, the amount of memory wasted by fragmentation can rise over time, slowly deteriorating the performance of the whole operating system. In extreme cases, the allocator can run into problems by exhausting the whole available memory, while having a large amount of unused memory.\cite{DSAsurvey} A repeated allocation scenario from the last paragraph on the heap-based allocator could lead to such situation.

\section{Variables influencing memory allocator performance}
In previous section, various metrics having a relevance as performance ones were described. This section discusses factors influencing these metrics. Knowledge of these factors is needed in order to provide a framework for allocation scenario creation.
\subsection{Systemcall costs}
\subsection{Size of allocation}
\subsection{Allocation and deallocation order}
\subsection{Heap size}
\subsection{Threading}
\chapter{Benchmarking methodology}
%\chapter{OSMB tool}
%\chapter{Different malloc implementations analysis}
%\chapter{Conclusion}
%I will be Ing. \cite{Pravidla}

%=========================================================================
