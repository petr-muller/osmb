%=========================================================================

\chapter{Introduction}

Memory usage is one of the most fundamental principles in computing, both historical and modern. Every program needs some storage - if only for storing itself somewhere. Modern computing environments contain lot of memory layers on different levels. There is persistent storage (harddisks, SDD drives), operating memory, several levels of CPU caches and registers. Usage of all of these is usually abstracted by the operating systems, which provide the safe access to these resources to the userspace applications. Technical advancement in recent age caused the prices of hardware providing storage to go down, so in most causes totally effective usage of these resources is not an outstanding issue when programming. The storage is usually relatively cheap to the price of work an expert programmer has to do to exploit the resources effectively. However, sometimes the effectiveness matters, especially in some specific areas. Some major projects are suffering by the performace hit introduced by the currently used allocators, and invested time and resources to the creation of new allocators which will solve their problem. The example of this is Google writing TCmalloc for improved performance of multithreaded applications, or OpenLDAP project considering changing the allocator from ptmalloc2 (the glibc default) to improve the performace \cite{openldap-pres}. This thesis attempts to provide a tool for an analysis of various memory allocators' characteristics under different usage scenarios.

\section{Motivation}

Dynamically allocated memory is a memory allocated not during compile time, but on runtime. The reasons vary---the amount of memory needed can be computed from variables known at runtime, or the memory is needed just for the limited amount of time and so it would be ineffective to hold it for the whole runtime. The concept is almost as old as modern computing itself. It seems to be present and used since the 1960s \cite{DSAsurvey}. In C language, which is still mostly used in system programming in Linux world, this concept is present in the omnipresent malloc/free interface \cite{glibc-man-malloc}. Of course, this interface is present and widely used for a very long time, so the concepts are already researched thoroughly. There are some very efficient allocation algorithms in currently used implementations of malloc. There seems to be little space for some radical improvements.

However, the effectiveness of a memory allocator is also largely influenced by the usage pattern of it. Over time, the most used memory allocator implementations converged towards an implementation which has at least an acceptable behavior for most of the usage cases. This means that for most cases, a program has a good probability to perform well with the allocator, but it could do better with another. Most widely used allocators often provide a way for fine tuning them for the particular environment \cite{glibc-man-malloc}, but often the results are not as good as they could be if the allocator was made specifically for the specific usage pattern, sacrificing performance of another cases.

The good example of this case is the program concurrence. For handling multiple threads allocating memory concurrently, the allocator has to have some mechanism avoiding conflicts, like locks. An universal allocator does not know whether program using it is concurrent or not. A lot of programs use threads, so it needs to have some overhead for them. This overhead is completely unnecessary for single-threaded programs, and better performance would probably be achieved if an allocator without the multi-threaded overhead was used. Such an allocator of course could not be used as system-wide, universal one. All of this is of course even more complicated in reality.

The environment also changes in time. The appearance of multi-core CPUs brought some specific problems with cache usage. It is better to allocate memory in a way that is cache friendly, that means to arrange memory which is used closely in time also close spatially, and to avoid writing into memory which is shared among the CPU caches. When CPUs invalidate each other's caches a lot, the advantage of having it is lost. Such problems can be avoided with proper memory management.

The advantage of using tuned or alternative memory allocator is often not great enough to justify additional work. Stock allocators are quite effective in most cases. But there are cases in which the gain can be quite high. High enough so we can see a major software gigant creating it's own malloc implementation to solve their problems with existing implementation \cite{tcmalloc}. Another cases can be seen in some object-oriented libraries, which implement additional layer of memory management. The reason is that creating objects does quite a large number of small allocations, which is not effective enough. So these libraries keep a pool of preallocated objects on which they do some amount of memory allocation of their own \cite{glib-memalloc}.

This thesis aims to help with analysing performance of various memory allocator implementations (and their tunings) by providing a tool which can measure different performance aspects under various conditions. At the moment, such an analysis seems to be usually done by micro benchmarks crafted for the actual need without systematic approach, at least in the open-source software world.

\section{Goals}

The goal of this thesis is to provide a tool for analysing memory allocator performance. Allocator performance is a vague term - different metrics can be claimed as important in different situations. The tool should be able to measure all of the most common ones. These will have to be identified. Also, there are many factors which influence these metrics. The tool shall allow the user to specify these factors to make the measurement relevant for the particular situation.

After the identifying these factors and metrics, the tool will be implemented and test analysis will be performed on various memory allocation implementations. This term project does not contain the implementation of this tool, that will be done in the thesis. Just the high-level design decisions will be made based on the identified metrics and factors.

\chapter{Dynamic memory allocators}

This chapter discusses the principles of dynamic memory allocation system used in modern GNU/Linux systems. Because {\em performance} is very unclear term when used in context of allocators, various metrics are analysed as performance ones, and factors which influence these metrics are discovered.

\section{Dynamic memory allocation}

Dynamic memory allocation is an ubiquitous concept in current programming world. It originated in the very dawn of modern computing, in 1960s, and was driven by the need of using the memory resources as effectivelly as possible. Dynamic memory allocation occurs at run-time, where a running program asks the operating system (or more generally, the memory resource manager) for more memory for its needs. This is a direct opposite of compile-time {\em static} allocation. There are more reasons for postponing allocation to runtime. The most significant ones are:
\begin{enumerate}
\item The exact amount of needed memory is simply not known during compile-time. This is the common case: the program simply does not know how long e.g. the input will be.
\item The program needs various amounts of memory in time. Such a memory could possibly be allocated at compile-time if the size is known: simply the maximum of the memory consumption would be allocated. That would mean wasting the memory, because the program would have allocated a lot of memory even when it uses all of it for only a small portion of runtime.
\end{enumerate}

Over time, the specifics of the memory usage by programs changed. Historically, the memory of the computers was very small and expensive, so the need was to place the data in it as effectively as possible. Today, the needs are different. It is still a good habit to use the memory effectively, but it is no more that crucial, because the price of the memory dropped and the capacity rised dramatically. But the advancement also caused several new problems to emerge. Modern computers generally have enough of operating memory: the problem is, that the memory access is very slow compared to the speed of modern CPUs. This is solved by the fast memory layers in CPU: caches. But fast memory is more expensive, so the cache sizes are very small when compared to the main operation memory size. If a program does not use the cache well, it's performance can be drastically slower than the performance of one that does \cite{drepper-memory}. The way how the dynamic memory allocator works can affect also the cache friendliness of the programs---by well placing the memory which has a good probability of being used together. Similar to this, a bad memory allocator can also introduce a performance hit if it places the memory badly.       

\subsection{DSA specification}

In the scope of this thesis, the term "dynamic memory allocator" means the usual malloc/free interface as it is known from the C programming language. The programs use the allocator for obtaining and returning arbitrary amount of storage space for its own use. These allocations and allocations can occur at any time and in any order. Allocated memory can be directly addressed by the program - the access to the storage is done outside of the allocator scope. The allocator is also completely unaware about what is stored in the allocated memory. This means that the allocator cannot do any internal management on already allocated blocks of memory, e.g. moving the allocated blocks so that they are stored compactly, or possibly compressing them. If such operations were done by the allocator, the program references to such memory blocks would be rendered invalid, and such an effect is of course indesirable. The direct access also means that the allocated memory chunk needs to be continuous, as opposed to e.g. filesystem storage management. The program cannot recognize the hole in the storage without the allocator's assistance - so the hole simply can not exist. {\em Continuous} means continuous in the program's address space here. Tt can be assembled from the several pages at the level of virtual memory in kernel of course---this is totally transparent to the program, including the allocator itself. 

\subsection{DSA concepts}

The allocator operates on some amount of memory, which is usually (but not necessarilly) contiguous. Some part of this memory is used for the memory allocator data - these will be called metadata later in this thesis. The rest of this memory is used for provisioning to a program, and is usually divided into some number of blocks of various size. When a program makes a request for some amount of memory, the allocator finds or creates some free block of sufficient size, marks it as used and gives it's address to the program. If no large enough block is found, additional memory can be obtained from the operating system using appropriate system calls, such as {\em mmap}, {\em brk} or {\em sbrk}.\cite{sbrk-manpage, mmap-manpage} When a memory is freed by the program, the block is marked as free. Free blocks may be used to satisfy subsequent memory request, or it may be returned to the operating system. The goal of an allocator is to minimize the unallocated memory amount, while minimizing also the time cost of this effort.

There are few notable constraints which makes the allocator's goal harder. The allocator cannot control the number, order and the memory size of allocations - these are set by a program using the allocator. The allocator also satisfies one allocation at a time (with the exception of possible concurrency, which does not affect this problem), without the knowledge about the future allocations requests. The memory must be provided immediatelly, and once the allocator selects the memory place for it, it cannot be modified when additional requests come and better decision could be made, until the program frees that memory. It can be proven that it is not possible to create a dynamic memory allocator which will not waste memory by bad decision about chunk placement.\cite{DSAsurvey}

Two main concepts exist in dynamic memory allocation. The first one uses the heap exclusively, and a {\tt brk} system call to expand and shrink it. The second one relies on the {\tt mmap} system call, and uses the anonymous memory outside the heap. Some allocator are using exclusively one of these concept, while some others use the combination of both. The GNU/Linux family of operation systems usually come with the GNU C library (usually abbreviated as {\tt glibc}). This library uses a variant of Doug Lea's dlmalloc called ptmalloc2. The improvements are mostly the for better performance in multi-threaded applications (the "pt" prefix comes from the term {\em pthreads}, a commonly used abreviation for the POSIX Threads standard). This allocator uses a combination of the two concepts - it uses heap for smaller requests and anonymous memory for the larger ones. Aside from this widespread memory allocator, there are several more alternative memory allocators used on Linux, usually claiming better performance in various aspects. The two more popular are Hoard and TCmalloc. Hoard, developed at the University of Massachusets in Amherst, is a thread oriented memory allocator using {\tt mmap} exclusively to construct an internal architecture of per-thread heaps.\cite{allocators:hoard} The second one is also a thread oriented allocator, called Thread Caching malloc - TCmalloc. TCmalloc uses {\tt mmap} exclusively and has quite sophisticated system of allocation based on different allocation strategies based on the request size. It also provides a thread local cache to provide quick allocation for small requests to avoid locking.\cite{tcmalloc} TCmalloc is developed by Google, Inc. Few more experimental alternative memory allocators exist, but these does not seem to be used much.

\section{Interesting performance metrics of memory allocators}
\label{metrics}

There is no clear performance metric for the memory allocators. There are different metrics describing the characteristics of an allocator, and their importance and relevance as a performance metric depends on the characteristics of a program using an allocator. For example, a time per allocation certainly matters in a program doing millions of allocations, while it is not much interesting in a program with only few allocations. This section lists various metrics which can be under some circumstances considered as relevant performance metrics, along with these circumstances.

There are two types of performance metrics: time and memory usage. Time metrics affect the program execution speed in some way. Two time-affecting metrics were identified: allocation speed, which directly affects the program speed, and locality, which affects the program speed indirectly by making the program more or less friendly to CPU cache systems. Memory metrics are also two: fragmentation and memory overhead. Memory overhead increases memory usage by using some memory for the allocator data. Fragmentation affects the efficiency of using memory for provisioning to a program.

\subsection{Memory overhead}
\label{metrics:overhead}
Every memory allocator needs some memory for it's own use. At the very least, it needs to store the sizes of the memory blocks provided to program, so they can be marked as unused when freed. There are two possible types of memory overhead: per allocator, and per allocation. Overhead per allocator means a memory consumption by {\em global} data structures used by the allocator. This memory will be used even when no memory is allocated by the program. Some allocators need some metadata memory for each allocation made by the program. This is per-allocation overhead. The former is not that important, if it is not extremely large. The latter can be important under certain conditions. One of the cases when per-allocation overhead is interesting metric is obviously when program does a large amount of very small allocations. For example, when per-allocation metadata needs 16 bytes and the program does a lot of 16 bytes allocations, the amount of memory needed for such program is twice as high as the amount actually used by it. There are more reasons why the allocator can create some overhead memory: in addition to the memory actually used by allocator metadata the memory overhead doesn't necessarily need to be used at all. The free unused memory block can be created when some architecture dependent decisions are made, for example alignment. A figure \ref{fig:overhead} shows the different types of memory overhead. Memory overhead is sometimes considered to be a type of fragmentation. Fragmentation is discussed in more details in \ref{metrics:fragmentation}.
\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth,keepaspectratio]{fig/frags}
\end{center}
\caption{The difference between overhead and fragmentation}
\label{fig:overhead}
\end{figure}

\subsection{Allocation speed}

Allocation speed is simply the amount of time spent processing the allocation or deallocation request. In usual use cases, the allocation speed is not very important, because allocator requests usually make only a minor portion of program statements. In such cases, time taken by memory allocation requests is negligible in the total time the program is executing (if the time taken by allocator is not extreme, of course), so the performance impact in the program is low. However, this is not always the case. In some environments where a large number of allocations and deallocations occurs, the program performance can rapidly increase and decrease in accordance to the allocator performance. An example of such environment is the interpreter of some object-oriented scriptiong language, such as Python or Ruby. When these interpreters are processing some scripts, large hierarchies of objects can be created or destroyed (for example when entering or leaving a function), which results in a large chain of small allocations or deallocations. Low performance allocator can rapidly decrease performance of execution of such a program.

\subsection{Locality and spatial layout}

Locality is a metric which can dramatically affect performance of a program. Current computers have several layers of operating memory - in addition to "normal" operating memory, current CPUs have several levels of cache memory. For example, Intel i7 family of CPUs have three levels of cache. Usually, the closer the cache level is to the CPU itself, the faster the operations with data in it. Reading data from L1 cache can be 80 times faster than reading same data from the main memory. For L2 cache, it is cca 17 times \cite{drepper-memory}. Smart memory allocator can arrange data in a way that is friendly to this memory structure: if data, which are often used together are also placed near themselves in memory, then it is likely that both can be present in the fastest cache, and thus all the work can be done on the fast cache level without the need of multiple main memory accesses. Similarly, poor allocator can place data which are used together in different memory areas, which then basically eradicates the advantage of having a cache because of cache trashing.

Allocators can improve their locality metric either by taking advantage of some usual program patterns, or by taking hints from the program it serves. One quite common pattern is a temporal one. Data which are allocated close in time are quite likely to be used close in time. So if an allocator places temporally close allocations also spatially close, it has a good chance of being cache-friendly, improving the program's performance. Second method is taking hints from the program itself - the programmer usually knows which data will be used together.

Second issue concerning memory layout is the allocator induced false sharing. False sharing is a usage pattern where two threads use their objects exclusively, but theses two objects share a cache line. When one thread modifies its object, second one will have to read its object from memory again, despite the object was not modified and it could be safely read. The reason for this is that when the first CPU wrote its data, whole cache line was rendered as invalid for the other CPUs. If the allocator places the memory chunks allocated by different threads close to each other, it can lead to lower program performance because of the false sharing caused by this.

\subsection{Fragmentation}
\label{metrics:fragmentation}

Fragmentation is a metric representing the effectivity with which the allocator manages it's memory. The allocator has some amount of memory disposable. The difference between this amount and the amount actually used (as in provided to the program, it's not important if the program actually uses its allocated memory) by the program gives us fragmentation. Fragmentation is usually taken in a form of fragmentation ratio or percentage, which is computed like $unused\_memory / total\_memory$ and says how much of the memory is not used. The goal for an allocator is to have this metric as low as possible.

In literature, fragmentation is often divided into two categories: internal and external.
\begin{enumerate}
\item Internal fragmentation is memory actually used for something, but not accessible to the program. Internal fragmentation was discussed in \ref{metrics:overhead} and in this thesis the term {\em memory overhead} is used for this type of fragmentation.
\item External fragmentation is memory, which is for some reason currently unallocated by the program. In his thesis, the term {\em fragmentation} is used for this kind of fragmentation. The reason for dividing these two types with distinct names is an attempt to avoid confusion. This thesis works with both these metrics in different ways, because they are affected by different factors. This subsection later deals just with this metric.
\end{enumerate}

The reasons why fragmentation come up can be different. It can be a memory already obtained from the operating system, but not yet allocated by the memory, for example at the start of the program. This can be a case where an allocator is handling lot of small allocation requests - explicitly asking the operating system for tiny amount of memory is unefficient: the syscalls are usually quite expensive. Therefore, allocator requests a larger block at the beginning and uses pieces of this block to satisfy the allocation requests. Also, there can exist a situation where free memory exists when an allocation request is made, but for some reason cannot be used to satisfy it. The reasons for this vary: the block can be free space between two used blocks, too small for the allocation request to be satisfied. Or, in a multithreaded environment, allocator can have a sufficiently large free block of memory, but it doesn't use it because it only uses that memory chunk for some exact thread, not the one requesting the memory.

The problem with fragmentation is that it is not a proper feature of an allocator. The major factor affecting fragmentation is how the allocator is used, i.e. size and order of the allocation requests. For some work loads fragmentation of a certain allocator can be low, and high for other. Two allocation scenarios on a simple heap-based allocator illustrate this. Simple heap-based allocator simply grows the heap if asked for memory, and shrinks it if asked to free memory chunk at the top of the heap. For allocation requests coming as {\tt malloc} paired with subsequent {\tt free} of the same memory, no fragmentation would occur. The heap would grow with {\tt malloc} and then it would shrink after free. But after the sequence {\tt malloc(ptr1, 100), malloc(ptr2, 5), free(ptr1)} there is large fragmentation: the heap size is 105, but only a memory chunk with size 5 is actually used. Note that this is not so uncommon pattern in software. A similar behaviour could be observed for a program part looking like this: {\em allocate space for data; allocate space for result; do some work on data; store the result; free the data; use the result}. The behavior of the described allocator on these two scenarios is showed on figure \ref{fig:fragmentation}.

\begin{figure}[h]
\begin{center}
\includegraphics[keepaspectratio,width=1.0\textwidth]{fig/fragmentation}
\end{center}
\caption{Emergence of fragmentation in two scenarios}
\label{fig:fragmentation}
\end{figure}

It can be clearly seen that fragmentation can vary even in different times during the execution of a single program. This can be a problem especially for a long running programs doing lot of allocations and deallocations, like daemons or web browsers. If a bad memory allocator is used, the amount of memory wasted by fragmentation can rise over time, slowly deteriorating the performance of the whole operating system. In extreme cases, the allocator can run into problems by exhausting the whole available memory, while having a large amount of unused memory.\cite{DSAsurvey} A repeated allocation scenario from the last paragraph on the heap-based allocator could lead to such situation.

\section{Variables influencing memory allocator performance}
\label{variables}
In previous section, various metrics having a relevance as performance ones were described. This section discusses factors influencing these metrics. Knowledge of these factors is needed in order to provide a framework for allocation scenario creation.
\subsection{Systemcall costs}

When an allocator needs to provide more memory, but it has no suitable free space available, it has to ask the operating system for more memory. This is done by appropriate system calls. Linux kernel provides these two system calls suitable for this purpose.

\begin{enumerate}
\item {\tt brk} system call sets the end of program's data segment to some certain address. It can be used to enlarge or shrink the segment.\cite{posix, sbrk-manpage}
\item {\tt mmap} system call maps certain amount of memory into the process address space. This is usually used to map a file to memory, so it can be read and written via means of normal memory operations, not reads and writes. But (using the MAP\_ANON flag) it can also be used to allocate anonymous memory, which is not associated to any specific file.\cite{posix, mmap-manpage}
\end{enumerate}
Additionally, there is a {\tt sbrk} function, provided by C library. This is basically a wrapper over {\tt brk} system call, providing interface for relative (as opposed to absolute in the case of {\tt brk}) adjustment of data segment break.\cite{sbrk-manpage}

System calls are usually considered as expensive operations (due to control transfer to the kernel and back to userspace), so allocators usually try to map several allocation requests to single system call and doing internal bookkeeping on the memory. For this to be effective, the bookkeping must take less time then it would take so do a simple syscall.

System call cost is a factor affecting the allocation speed. The size of this influence depends on how often the allocator uses system calls. Allocator doing one huge memory request once in a while (with bookkeeping on the memory) will depend on system call much less then one which does a system call for every allocation request.

\subsection{Size of an allocation}

Size of an allocation can theoretically affect allocation speed, fragmentation and spatial layout/locality metrics. The impact on speed should, in theory, be quite minor. It should be slightly easier to find a suitable place for a small chunk request for a big one, but the allocator usually knows how large chunks it can provide at the moment, and for how large more memory should be requested from the operating system.

The impact on memory layout and fragmentation is higher. A larger chunk request means less possible places to put it into. This is limiting in how "well" the allocator can place the chunk. The allocator can have a lot of free space available, but if a request for a chunk just as large as the largest free continuous space comes, the allocator has only two choices. First choice is to place the chunk into the free place, possibly worsening locality or memory layout. The second one is to get more more memory from the operating system to place the chunk well from the locality point of view, but increasing fragmentation.

\subsection{Allocation and deallocation order}

The allocation and deallocation order has a major impact on how the fragmentation metric will look like during the usage of an allocator. As it was described in subsection \ref{metrics:fragmentation}, some combinations of allocation size and their ordering can cause "holes" in the memory managed by the allocator.

\subsection{Threading}

Multithreaded programs are bringing the whole new level of complexity into the memory allocation system. Threading (and distribution of memory allocations across threads) can affect all of the performance metrics described in section \ref{metrics}. 

The memory allocator can be thought as a resource for the program. In a multi-threaded program, any thread can request to allocate or free memory at any time. If an allocator wouldn't be enabled for multithreaded environment, race conditions would probably occur, resulting into several threads using the identical memory area and rewriting each other's data. To avoid this, the allocator has to have some kind of control. Such a control could be a locking scheme, which would serialize the allocation requests of threads. The scheme can be one large lock, or a finer grained locking scheme, introducing a pipeline. Another approach is to create an internal thread specific "arenas". Every thread has it's own "arena" in the allocator, and it's requests are satisfied by using memory exclusively from it, which eliminates a need for locking. This second approach is used in the GNU C Library's malloc.

The impact on allocation speed is obvious, and has two sources. Locks in the allocator cause the threads asking for memory to wait for each other, which could introduce a major speed reduction of allocation if threads are doing lot of memory requests. The second source is the overhead of the multi-threading capabilities of the allocator itself. Both locks and the thread specific "arena" mechanisms have some overhead, which would be unnecessary in single-threaded environment.

Second metric where the number of threads has some relevance is the memory layout and locality. Measuring the false sharing prone allocations only makes sense in multithreaded environment, because false sharing can only occur when a program runs simultaneously on at least CPUs. Locality metric should be measured inside single thread, and in each thread in multithreaded environment.
\chapter{Benchmarking methodology}
\label{methodology}
In chapter \ref{metrics}, several metrics with some relevance to the performance of an allocator were analysed and described. This chapter will discuss the methodology of measuring these metrics.

\section{Measuring memory overhead and fragmentation}
\label{methodology:fragmentation}
The benchmarking tools should be agnostic to the measured memory allocator, in order to be useful as an universal memory allocator benchmark. This means it should not depend on any internal knowledge of the allocator, nor it can use some allocator specific APIs. Using these would mean the benchmark would be unusable for analysis of the allocators which do not implement these APIs. The benchmark can only use the standardized {\tt malloc} and {\tt free} interfaces, with perhaps the future addition of {\tt realloc} interface. In addition to these APIs, the benchmark can also use some external tools to observe the behavior of an allocator.

This brings a problem with measuring the memory overhead of an allocator. All the means benchmark can use only observe the communication of an allocator with the environment. The benchmark only knows two things. It knows how much memory is actually allocated by the program. It also knows how much memory the allocator manages: the benchmark can intercept and log the memory requesting system calls or it can analyse the memory image of a process to discover the size of the heap. From these two numbers, the benchmark can compute how much memory is not used in program, but is granted by the operating system. There is no universal mean how to discover how much of this unused memory is actually memory overhead (and is used by the memory allocator metadata) and how much is fragmentation (and is not used at all). This information is a completely internal in the allocator. Some memory allocators provide the APIs to determine this information, but it is not universal, so the benchmark cannot use it.

At least fragmentation is very important metric, so because the benchmark cannot reliably measure these two metrics separately, the bechmark has to measure at least some information which will give some information about the fragmentation. The approach chosen is to measure just the total amount of memory which is not used by the program. This value is a sum of fragmentation and memory overload. This does not give a precise information about any of these metrics, but gives an information about how effectively the allocator manages it's memory. After all, from the outside, these two metrics share the most important characteristics: they both represent an amount of memory, which was granted by the operating system, but cannot be used in the program. This new {\em unused memory} metric's absolute value can be taken after each allocation and deallocation. From these values, a unused memory ratio can be calculated and statistically processed.

In addition to this, a real fragmentation metric can be calculated by detailed analysis of allocation and deallocation requests, and detecting if such request caused a system call. If an allocation request was done and satisfied without an system call being issued, it is certain that at the time of an allocation request the memory was free and unused in the allocator's managed area, and therefore considered as fragmentation. If another allocation request is made, and no system call is still caused by it, we can add this value to the previous one: at the time of the previous allocation request at least the space of the sum of these two allocations were fragmented for sure. When a system call for more memory is issued, the knowledge about fragmenation is invalidated and the computation has to start again. A system call returning memory to the operating system does not invalidate this value. Similar thing can be observed for deallocation requests. If a deallocation request does not cause an allocator to return the memory to the operating system, it is kept in the allocator and considered as fragmentation. Subsequent cases can be summarized too, and invalidated by the system call returning memory. This gives a limited information about fragmentation - it is a minimal amount of fragmentation in that precise time. Usefulness of this metric is limited to the concrete usage scenarios and allocator characteristics: for most of the scenarios mixing allocation and dealocation requests these values will be invalidated often, limiting their precision.

\section{Measuring allocation speed}

Measuring the speed of a single allocation is not relevant enough. In a multitasking environment, there are many side factors which can affect the allocation speed. The time of a single allocation is also very small, meaning any side effect will affect the measurement by a large mean and bring considerable error to the measurement. To obtain at least some precision, the measurement method should be statistically sound. This means a sufficiently large set of the identical experiments has to be done. The results of this set should be processed statistically. Second issue to be considered is the differentiation of time spend in userspace (which means in the allocator itself) and in the kernel, doing work in the system calls. These two time metrics should be taken separately. While the time spent in an allocator is a interesting metric by itself, the system call costs are not. System call costs are considered as an input in the system: they can change and the allocator has no control on how long the system call will take. The tool has to allow the user to specify also the system call costs in the environment, to find out the predicted allocator performance in such an allocator. This cost should be implemented as a function of the allocation size, with the possibility to incorporate some amount of pseudo randomness into this function. At least the means to specify standard and uniform distribution should be provided. With these abilities, it should be possible to describe the most important characteristics of the system call cost: dependence on the system call allocation size, constantness and some random changes in behavior of the kernel.

The exact method of measuring duration of a single allocation request should be saving the time (or the contents of the clock counting registers) first before the allocation, second after the allocation. The difference between these two numbers yields the total time spent in allocation, including the time spent in the kernel. The time spent in the kernel should be traced via an external tool. There are also some external effects present, so the resulting time will not be precise enough, but it could be useful e.g. in comparison done in the same environment.

There is another problem in measuring time. There are also context-switches, which can prolong the measured times. In some cases, these durations are interesting: time spent waiting in the kernel is interesing in the case of multiple threads doing allocations using an allocator with locks. Locking will slow down the execution of the program. This effect is interesting enough to measure. For this purpose, the tool should provide the ability to measure the execution not by per-allocation basis, but also on total program execution basis. This will incorporate also the effect of lock contention by threads. Doing statistical analysis of the sufficient number of experiments, the effects of any accidental interference (context switches to another programs and similar) should be mitigated in the resulting value.

\section{Measuring locality and memory layout}

Analysis of the memory layout should not be very hard. The tool has complete information about the positions of the allocated memory as well as the size it. Using this information, it can construct an image of how the memory looks like. From this image, it's easy to see how far from each other the allocated memory chunks are. There is also a drawback: because of virtual memory mechanism in place, the benchmark cannot reliably tell if chunks close to each other in the process' address space are close even in physical memory. The process' continuous memory space can be assembled from several fragmented physical memory pages. So the proximity in process space does not imply the proximity in physical memory, and by extension, the real locality. But the locality would be broken only on the boundaries of the physical memory pages. Most of the allocated data should be inside the pages, which means the better proximity in process space means higher probability of having better real locality.

Inside single thread, the measured metric should be the locality of two allocated chunks of memory based on the temporal proximity. The absolute distance between two chunks of memory can be computed as a difference between the end of the chunk with higher address and the address of the chunk with lower address. The absolute distance depends on the sizes of these two chunks, so for the purpose of having comprarable we will relativize it:
\begin{displaymath}
DISTANCE_{rel} = \frac{size_1 + size_2 + gap}{size_1 + size_2}
\end{displaymath}
The numerator of the fraction is the absolute distance, and by dividing it by the sum of size, we obtain a relative locality metric. A value of 1 means there is no gap between the two chunks, meaning the best locality. The higher number means worse locality. This metric will be taken for each pair of a specific temporally distant allocation. This will allow to compute the average locality of each temporal distance - temporal distance 1 means the allocation requests were issued one just after the other, distance 2 means the two allocations had one more between them, and so on.

Across multiple threads, a similar metric should be taken for determining of the possibility of false sharing. The closer the two memory chunks allocated in different threads, the higher is the probability of false sharing if these two chunks were used simultanneously in their owner threads. For each two threads in the system, the tool will find the sizes of gaps between the chunks belonging to different threads. From the detailed analysis of sizes of these gaps it will be possible to find out the minimal gap size to determine of any false sharing can occur at all for the given cache line size, as well as the number of memory chunks which could cause false sharing if used simmultaneously.

Another approach to analyse locality in a rather practical way is to do some computing on the allocated memory, and observing cache misses on various levels occuring during the runtime. In multithreaded environment, the misses count will probably heavily depend on the execution order of the threads, which means more experiments will have to be done for obtaining numbers with higher precision.

\chapter{OSMB toolset}

This chapter will discuss the design and implementation of the tool. OSMB is a abbreviation of "Open Source Malloc Benchmark".

\section{Design}

The toolset will be composed from several smaller tools which will measure various metrics as described in previous chapter. This approach was chosen because of the better precision of the measurement. If all the metrics were taken during one benchmark run, some metrics, especially the time ones, could be affected by the overhead introduced by the benchmark. At least for the time performance metrics the benchmark tools should have as low overhead as possible. In contrast to this, for fragmentation and memory layout analysis there is no need for this, because the goal is to obtain precise results even for the price of some duration overhead.

To allow the user to specify the environment for testing, i.e. the values of the various factors as described in \ref{variables}, the tool will take a {\em scenario file} as an input. This file will be written in a domain specific language, and will specifify how all of these variables, or a subset of them will look like in the environment. This file will serve as input to the {\em scenario driver}, which will parse it and will execute one run of such a scenario. During the execution, one or more measurement tools will attach to the driver, and will take the metrics. The tool should allow the user to run the scenario for the specified number of times, obtaining results from all of these runs, and then perform some statistical analysis of the collected data. To mitigate the effect of "warming up", the tool should provide the ability to run the arbitrary numbers of unmeasured executions.

There are two sources of the scenario input file. The format will be text based domain specific language, so the user can manually write the scenarios, or write a generator. One such generator should be provided with the tool. The generator should be able to attach to any running process which uses dynamic memory allocation, and trace it's usage of {\tt malloc} and {\tt free}, creating a scenario file from the collected information. Tracing these functions will be done by using external tools: {\tt ltrace} or {\tt systemtap}.

The measurement tools should not do any unnecessary computing during the benchmark execution. They should only record the observations. These raw data should be processed after the benchmark execution. The toolset should also provide some tools for processing, interpreting and visualization of these raw data.

The measurement tools' function will we be direct implementation of the principles in chapter \ref{methodology}. The design for the remaining parts of the suite is described in the following subsections.

\subsection{Scenario driver}

The scenario driver will be the central piece of the whole suite. It will take the scenario file as an input, parse it and create a program image of the scenario by the instructions in the input file. It will then hook the specific measurement tools into this generated program. The result will be compiled using standard C compiler. The resulting binary program is the representation of one run of the described scenario. This scenario will be run multiple times, and during its execution the external observation tools will attach to it. The output of these external tools, combined with the output of the scenario binary itself is the result of the benchmark.

\subsection{Tool for system call catching}

Some of the metric measurement tools will benefit from the system call observations. There are two kinds of system calls which can occur during the benchmark execution. First are the memory allocation related system calls. These should be recorded with the parameters showing how much memory was processed by them. The rest are the system calls unrelated to memory management. These will be not recorded at all. If it would be possible to record only the system calls coming from the allocator itself, and not e.g. the measurement tools, maybe it would be useful to track some others, like the locking related ones, like {\tt futex()}. This is easily achievable with existing tools, like {\tt strace}, so the focus of this work won't be on this use case.

\subsection{Scenario recorder}

The recorder will attach to any running program and trace it's usage of memory allocation functions. Tools like ltrace\cite{ltrace}, systemtap\cite{systemtap} or thethirdoneTODO\cite{TODO!} will be used for this. The data collected from this trace will be translated into the scenario file, and can then be used independently on the original program. Analysis of such a scenario can be used to tell, if some non-standard memory allocator would improve the performance of the original program. The recorder will have to be able to distinguish the memory allocations requests done by multiple threads, in order to correctly replicate the memory allocation scheme of the recorded program.

\subsection{Data analysis tools}

The output of the benchmark run will have the form of the raw data, with no particular information in them. The suite will also provide several tools to help the user analyse and interpret these raw data. The tools should provide tools for finding at least these outputs:
\begin{itemize}
\item Unused memory ratio. The maximal, minimal, average, median, mean, quartil values and standard deviation should be reported. Also, the graph showing the unused memory ratio per run should be provided. Additionally, a third graph should be reported, showing the progression of the unused memory ratio dependent on the allocation and deallocation request during the time of an execution.
\item Duration of one run. The maximal, minimal, average, median, mean, quartil values and standard deviation should be reported. The graph showing the duration per run should be provided. The purpose of this graph is to quickly recognize and compare the speed stability of the allocators. Such an instable allocator would have large standard deviation compared to the average values, and this can be easily seen in a graph. Additionally, a graph showing the predicted speed of one run dependent on the system call cost should be provided, with the actual average system call cost highlighted.
\item Fragmentation. The fragmentation metric found out as described in section \ref{methodology:fragmentation} should be reported just per one run. Because the number of measured values need not necesarilly stay the same in more executions of the scenario, the values are not easily comparable. Per one run, a minimal, maximal, average, median, mean and quartil values should be reported. The standard deviation should be also reported. In addition to this, the graph showing the progression of fragmentation during one execution of the scenario should be provided. Because the value of fragmentation can be unknown for some allocation and deallocation requests, the graph should highlight these allocations as unknown value to avoid confusion of misunderstanding of the graph.
\end{itemize}

All of the graph outputs whould provide the ability of showing the results of the several runs in one graph, providing easy comparison of them.
\chapter{Conclusion}

This term project had three main goals. This chapter describes the achieved work on these goals, and makes conclusions based on these goals.

\section{Achieved work}

The inital goal of the term project was to describe the dynamic allocation problematics, and identify the features and characteristics important to the topic of the thesis. These were described in chapter \ref{dsa}. Several performance metrics were identified and described. The identified performance metrics were memory overhead, memory fragmentation, allocation speed and memory locality and layout metrics. For these metrics, a benchmarking methodology was created. The second part of the initial goal was to find the factors affecting these performance metrics. These factors were described in \ref{factors}. The factors are these: system call cost, allocation request size, the order in which the requests are coming, and the number of threads in the application using the allocator.

Using the identified factors and performance metrics, the high-level design of the benchmarking tool was designed. The design was driven by the two main goals: it has to provide the ability to influence all of the performance factors in the benchmarking environment, and it has to be able to measure all the performance metrics as presisely as possible, using statistical means where necessary. The design also identified the precise output values for each metric. 

The whole tool was designed to be a interpreter of a domain specific language, encoding the usage scenario of the allocator: both the environmental characteristics (system call costs) and the program characteristics (threading, allocation size and order). The interpreter will create a C program. The execution of this program will then be analysed by the various metric specific tools, both during the execution (data collection) and after it (data analysis and interpretation).

\section{Future work}

Using the factors and metrics collected in this term project, as well as the initial high-level design decision, the benchmarking tool suite will be implemented. Aside from the implementation itself, the tool will use several external tools to do some specific tasks. It will use GCC to compile the programs. Systemtap will be used for the userspace-kernelspace communication interception, and if possible also to catch the malloc/free calls in the userspace. If this is not possible, ltrace will be used.

When the implementation is ready and working, the tool will be tested on several allocators. The main popular ones will be tested, and some interesting experimental one too. These candidates for the testing will have to be found. 
%\chapter{Different malloc implementations analysis}
%\chapter{Conclusion}
%I will be Ing. \cite{Pravidla}
%=========================================================================
