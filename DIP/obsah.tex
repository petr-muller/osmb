%=========================================================================

\chapter{Introduction}
Memory usage is one of the most fundamental principles in computing. Every program need some storage - if only for storing itself somewhere. Modern computing environments contain lot of memory layers on different levels. There is persistent storage (harddisks, SDD drives), operating memory, various levels of CPU caches and registers. Usage of all of these is usually abstracted by the operating systems, which provides the userspace applications safe access to these resources. Technical advancement in recent age caused the prices of hardware providing storage to go down, so in most causes the totally effective usage of these resources is not an outstanding issue when programming. The storage is usually relatively cheap to the price of work an expert programmer has to do to exploit the resources effectively. However, sometimes the effectiveness matters. This thesis attempts to provide a tool for an effective work with dynamically allocated program memory.
\section{Motivation} 
Dynamically allocated memory is a memory allocated not during compile time, but on runtime. The reasons vary - the amount of memory needed can be computed from variables, or the memory is needed just for the limited amount of time and so it would be ineffective to hold it for the whole runtime. The concept is almost as old as modern computing itself. It seems to be present and used since the 1960ties. In C language, which is still mostly used in system programming, this concept is present in the omnipresent malloc/free interface. Of course, this interface is present and widely used for a long time, so the concepts are already researched thoroughly. There are some very efficient allocation algorithms in currently used implementations of malloc. There seems to be little space for some radical improvements.

However, the effectiveness of memory allocator is also largely influenced by the usage pattern of it. Over time, the most used memory allocator implementations converged towards an implementation which has at least an acceptable behavior for most of the usage cases. This means that for most cases, a program has a good probability to perform well with the allocator, but it could do better with another. Most widely used allocators often provide a way for fine tuning them for the particular environment, but often the results are not as good as they could be if the allocator was made specifically for the usage pattern, sacrificing performance for another cases.

The good example of this case is the program concurrence. For handling multiple threads allocating memory concurrently, the allocator has to have some mechanism avoiding conflicts, like locks. Stock allocator does not know whether program using it is concurrent or not. A lot of programs use threads, so it needs to have some overhead for them. This overhead is completely unnecessary for single-threaded programs, and better performance would probable be achieved if an allocator without the multithreaded overhead was used. Such an allocator of course could not be used as system-wide, universal one. All of this is of course more complicated in reality.

The environment also changes in time. The appearance of multi-core CPUs brought some specific problems with cache usage. When CPUs invalidate each other's caches a lot, the advantage of having it is lost. Such problems can be avoided with proper memory management, which can be designed to be cache-friendly.

The advantage of using tuned or alternative memory allocator is often not great enough to justify additional work. Stock allocators are quite effective in most cases. But there are cases in which the gain can be quite high. High enough so we can see a major sotfware gigant creating it's own malloc implementation to solve their problems with existing implementation. Another cases can be seen in some object-oriented libraries, which implement additional layer of memory management. The reason is that creating objects does quite a large number of small allocations, which is not effective enough. So these libraries keep a pool of preallocated objects on which they do some amount of memory allocation of their own.

This thesis aims to help with analysing performance of various memory allocator implementations (and their tunings) by providing a tool which can measure different performance aspects under various conditions. At the moment, such an analysis seems to be usually done by micro benchmarks crafted for the actual need without systematic approach, at least in the open-source software world.
\section{Goals}
The goal of this thesis is to provide a tool for analysing memory allocator performance. Allocator performance is a vague term - different metrics can be claimed as important in different situations. The tool should be able to measure all of the most common. These will have to be identified. Also, there are many factors which influence these metrics. The tool shall allow the user to specify these factors to make the measurement relevant for the particular situation.

After the identifying these factors and metrics, the tool will be implemented and test analysis will be performed on various memory allocation implementation.
\chapter{Dynamic memory allocators}
\section{Dynamic memory allocation}
\section{Variables influencing memory allocator performance}
\subsection{Systemcall costs}
\subsection{Size of allocation}
\subsection{Allocation and deallocation order}
\subsection{Heap size}
\subsection{Threading}
\section{Interesting performance metrics of memory allocators}
\subsection{Memory overhead}
\subsection{Allocation speed}
\subsection{Locality}
\subsection{Fragmentation}
\chapter{Benchmarking methodology}
\chapter{OSMB tool}
\chapter{Different malloc implementations analysis}
\chapter{Conclusion}
I will be Ing. \cite{Pravidla}

%=========================================================================
